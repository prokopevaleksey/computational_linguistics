{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data parsing...\n",
      "data parsed!\n"
     ]
    }
   ],
   "source": [
    "# directories 'pos' and 'neg' sould be in the same folder as the code\n",
    "# here I just parse the data in a nice way\n",
    "# data is already sorted in the groups for cross-validation\n",
    "# I use 'cv9...' as a test and the rest as training data \n",
    "print('data parsing...')\n",
    "data_dir = ''\n",
    "classes = ['pos', 'neg']\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for curr_class in classes:\n",
    "    dirname = os.path.join(data_dir, curr_class)\n",
    "    for fname in os.listdir(dirname):\n",
    "        with open(os.path.join(dirname, fname), 'r') as f:\n",
    "            content = f.read()\n",
    "            if fname.startswith('cv9'):\n",
    "                test_data.append(content)\n",
    "                test_labels.append(curr_class)\n",
    "            else:\n",
    "                train_data.append(content)\n",
    "                train_labels.append(curr_class)\n",
    "print('data parsed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorisation for unigrams...\n",
      "vectorisation for unigrams is done!\n"
     ]
    }
   ],
   "source": [
    "# there is a good function in scikit-learn to vectorize data\n",
    "# I make unigrams with the words occuring more that 5 times and also cut words with frequencies upper 20%  \n",
    "print('vectorisation for unigrams...')\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True, ngram_range = (1,1))\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "print('vectorisation for unigrams is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training logistic regression classifier for unigrams...\n",
      "model is trained!\n"
     ]
    }
   ],
   "source": [
    "print('training logistic regression classifier for unigrams...')\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(train_vectors.astype(np.float), train_labels)\n",
    "prediction_lr = lr.predict(test_vectors.astype(np.float))\n",
    "print('model is trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.90      0.90       100\n",
      "        pos       0.90      0.89      0.89       100\n",
      "\n",
      "avg / total       0.90      0.90      0.89       200\n",
      "\n",
      "0.895\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.asarray(test_labels), prediction_lr))\n",
    "print(accuracy_score(np.asarray(test_labels), prediction_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorisation for bigrams...\n",
      "vectorisation for bigrams is done!\n"
     ]
    }
   ],
   "source": [
    "print('vectorisation for bigrams...')\n",
    "vectorizer2 = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf=True, ngram_range = (2,2))\n",
    "train_vectors2 = vectorizer2.fit_transform(train_data)\n",
    "test_vectors2 = vectorizer2.transform(test_data)\n",
    "print('vectorisation for bigrams is done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training logistic regression classifier for bigrams...\n",
      "model is trained!\n"
     ]
    }
   ],
   "source": [
    "print('training logistic regression classifier for bigrams...')\n",
    "lr2 = linear_model.LogisticRegression()\n",
    "lr2.fit(train_vectors2.astype(np.float), train_labels)\n",
    "prediction_lr2 = lr2.predict(test_vectors2.astype(np.float))\n",
    "print('model is trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.88      0.88       100\n",
      "        pos       0.88      0.89      0.89       100\n",
      "\n",
      "avg / total       0.89      0.89      0.88       200\n",
      "\n",
      "0.885\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.asarray(test_labels), prediction_lr2))\n",
    "print(accuracy_score(np.asarray(test_labels), prediction_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I decided to train my own w2v model from train data\n",
    "# also I formatted the original data in the nice way to train the model\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/a/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building data to feed to w2v...\n"
     ]
    }
   ],
   "source": [
    "print('building data to feed to w2v...')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "tokenizedbysentence = []\n",
    "tokenized_sents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nltk.tokenize.punkt.PunktLanguageVars()\n",
    "\n",
    "for j in range (len(train_data)):\n",
    "    tokenizedbysentence.append(tokenizer.tokenize(train_data[j].strip()))\n",
    "    tokenized_sents.append([a.word_tokenize(i) for i in tokenizedbysentence[j]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data are prepared to build a model!\n"
     ]
    }
   ],
   "source": [
    "sentences = [item for sublist in tokenized_sents for item in sublist]\n",
    "print('data are prepared to build a model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building a model\n",
      "model is built!\n"
     ]
    }
   ],
   "source": [
    "print('building a model')\n",
    "# for each word a vector with the size of 100 will be built\n",
    "# only words which occure more that 10 times will be used\n",
    "model = gensim.models.Word2Vec(sentences, workers=4, size=100, min_count=10) \n",
    "print('model is built!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building train and test data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('building train and test data...')\n",
    "train_vector3 = []\n",
    "for review in tokenized_sents:\n",
    "    sum_vector = np.zeros(100)\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                sum_vector = sum_vector + model[word]\n",
    "            except KeyError:\n",
    "                sum_vector = sum_vector + np.zeros(100)\n",
    "    train_vector3.append(sum_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data are ready!\n"
     ]
    }
   ],
   "source": [
    "test_tokenizedbysentence = []\n",
    "test_tokenized_sents = []\n",
    "\n",
    "for j in range (len(test_data)):\n",
    "    test_tokenizedbysentence.append(tokenizer.tokenize(test_data[j].strip()))\n",
    "    test_tokenized_sents.append([a.word_tokenize(i) for i in test_tokenizedbysentence[j]])\n",
    "\n",
    "test_vector3 = []\n",
    "for review in test_tokenized_sents:\n",
    "    sum_vector = np.zeros(100)\n",
    "    for sentence in review:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                sum_vector = sum_vector + model[word]\n",
    "            except KeyError:\n",
    "                sum_vector = sum_vector + np.zeros(100)\n",
    "    test_vector3.append(sum_vector)\n",
    "print('data are ready!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning a neural network with unigrams...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('traning a neural network with unigrams...')\n",
    "nn = Classifier(\n",
    "                layers=[\n",
    "                        Layer(\"Sigmoid\", units=1000),#increase the number of neurons of the 1st layer\n",
    "                        Layer(\"Sigmoid\", units=500, dropout=0.33),#increase the number of neurons of the 2nd layer\n",
    "                        Layer(\"Sigmoid\", units=250),\n",
    "                        Layer(\"Softmax\")],\n",
    "                learning_rate=0.0001,# you can play around with that too\n",
    "                n_iter=10000)#make it bigger if it helps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
       "      f_stable=0.001,\n",
       "      hidden0=<sknn.nn.Layer `Sigmoid`: units=1000, name=u'hidden0', frozen=False>,\n",
       "      hidden1=<sknn.nn.Layer `Sigmoid`: units=500, name=u'hidden1', frozen=False, dropout=0.33>,\n",
       "      hidden2=<sknn.nn.Layer `Sigmoid`: units=250, name=u'hidden2', frozen=False>,\n",
       "      layers=[<sknn.nn.Layer `Sigmoid`: units=1000, name=u'hidden0', frozen=False>, <sknn.nn.Layer `Sigmoid`: units=500, name=u'hidden1', frozen=False, dropout=0.33>, <sknn.nn.Layer `Sigmoid`: units=250, name=u'hidden2', frozen=False>, <sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
       "      learning_momentum=0.9, learning_rate=0.0001, learning_rule=u'sgd',\n",
       "      loss_type=None, n_iter=10000, n_stable=10, normalize=None,\n",
       "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
       "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
       "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(np.asarray(train_vector3), np.asarray(train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200, 2)]\n",
      "neural network is trained!\n"
     ]
    }
   ],
   "source": [
    "prediction_mlp = nn.predict(np.asarray(test_vector3))\n",
    "print('neural network is trained!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.66      0.59      0.62       100\n",
      "        pos       0.63      0.69      0.66       100\n",
      "\n",
      "avg / total       0.64      0.64      0.64       200\n",
      "\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.asarray(test_labels), prediction_mlp))\n",
    "print(accuracy_score(np.asarray(test_labels), prediction_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data for bigrams...\n"
     ]
    }
   ],
   "source": [
    "print('processing data for bigrams...')\n",
    "words = []\n",
    "for i in vectorizer2.get_feature_names():\n",
    "    words.append(i.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectors = []\n",
    "for bigram in words:\n",
    "    try:\n",
    "        bigram_vectors.append(np.concatenate((model[bigram[0]],model[bigram[1]]),axis=0))\n",
    "    except KeyError:\n",
    "        try:\n",
    "            bigram_vectors.append(np.concatenate((np.zeros(100),model[bigram[1]]),axis=0))\n",
    "        except KeyError:\n",
    "            try:\n",
    "                bigram_vectors.append(np.concatenate((model[bigram[0]],np.zeros(100)),axis=0))\n",
    "            except KeyError:\n",
    "                bigram_vectors.append(np.concatenate((np.zeros(100),np.zeros(100)),axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data are proccesed!\n"
     ]
    }
   ],
   "source": [
    "train_vector4 = []\n",
    "for vec in train_vectors2.toarray():\n",
    "    sum_vector = np.zeros(200)\n",
    "    for i in range(len(vec)):\n",
    "        sum_vector = sum_vector + vec[i]*bigram_vectors[i]\n",
    "    train_vector4.append(sum_vector)\n",
    "    \n",
    "print('data are proccesed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning a neural network with bigrams...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('traning a neural network with bigrams...')\n",
    "#nn1 = Classifier(\n",
    "#    layers=[\n",
    "#        Layer(\"Sigmoid\", units=10),\n",
    "#        Layer(\"Softmax\")],\n",
    "#    learning_rate=0.001,\n",
    "#    n_iter=25)\n",
    "nn1 = Classifier(\n",
    "                layers=[\n",
    "                        Layer(\"Sigmoid\", units=1000),#increase the number of neurons of the 1st layer\n",
    "                        Layer(\"Sigmoid\", units=500, dropout=0.33),#increase the number of neurons of the 2nd layer\n",
    "                        Layer(\"Sigmoid\", units=250),\n",
    "                        Layer(\"Softmax\")],\n",
    "                learning_rate=0.0001,# you can play around with that too\n",
    "                n_iter=10000)#make it bigger if it helps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
       "      f_stable=0.001,\n",
       "      hidden0=<sknn.nn.Layer `Sigmoid`: units=1000, name=u'hidden0', frozen=False>,\n",
       "      hidden1=<sknn.nn.Layer `Sigmoid`: units=500, name=u'hidden1', frozen=False, dropout=0.33>,\n",
       "      hidden2=<sknn.nn.Layer `Sigmoid`: units=250, name=u'hidden2', frozen=False>,\n",
       "      layers=[<sknn.nn.Layer `Sigmoid`: units=1000, name=u'hidden0', frozen=False>, <sknn.nn.Layer `Sigmoid`: units=500, name=u'hidden1', frozen=False, dropout=0.33>, <sknn.nn.Layer `Sigmoid`: units=250, name=u'hidden2', frozen=False>, <sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
       "      learning_momentum=0.9, learning_rate=0.0001, learning_rule=u'sgd',\n",
       "      loss_type=None, n_iter=10000, n_stable=10, normalize=None,\n",
       "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
       "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
       "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn1.fit(np.asarray(train_vector4), np.asarray(train_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vector4 = []\n",
    "for vec in test_vectors2.toarray():\n",
    "    sum_vector = np.zeros(200)\n",
    "    for i in range(len(vec)):\n",
    "        sum_vector = sum_vector + vec[i]*bigram_vectors[i]\n",
    "    test_vector4.append(sum_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200, 2)]\n",
      "network is built!\n"
     ]
    }
   ],
   "source": [
    "prediction_mlp2 = nn1.predict(np.asarray(test_vector4))\n",
    "print('network is built!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.64      0.49      0.56       100\n",
      "        pos       0.59      0.73      0.65       100\n",
      "\n",
      "avg / total       0.62      0.61      0.60       200\n",
      "\n",
      "0.61\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.asarray(test_labels), prediction_mlp2))\n",
    "print(accuracy_score(np.asarray(test_labels), prediction_mlp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Convolution, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building cnn for bigrams...\n"
     ]
    }
   ],
   "source": [
    "print('building cnn for bigrams...')\n",
    "cnn = Classifier(\n",
    "    layers=[\n",
    "        Convolution(\"Rectifier\", channels=10, kernel_shape=(1,10)),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.02,\n",
    "    n_iter=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(train_vector4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda2/lib/python2.7/site-packages/lasagne/layers/conv.py:489: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(batch_size=1, callback=None, debug=False, dropout_rate=None,\n",
       "      f_stable=0.001,\n",
       "      hidden0=<sknn.nn.Convolution `Rectifier`: channels=10, scale_factor=(1, 1), name=u'hidden0', frozen=False, kernel_shape=(1, 10), kernel_stride=(1, 1), pool_shape=(1, 1), border_mode=u'valid'>,\n",
       "      layers=[<sknn.nn.Convolution `Rectifier`: channels=10, scale_factor=(1, 1), name=u'hidden0', frozen=False, kernel_shape=(1, 10), kernel_stride=(1, 1), pool_shape=(1, 1), border_mode=u'valid'>, <sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>],\n",
       "      learning_momentum=0.9, learning_rate=0.02, learning_rule=u'sgd',\n",
       "      loss_type=None, n_iter=5, n_stable=10, normalize=None,\n",
       "      output=<sknn.nn.Layer `Softmax`: units=2, name=u'output', frozen=False>,\n",
       "      parameters=None, random_state=None, regularize=None, valid_set=None,\n",
       "      valid_size=0.0, verbose=None, warning=None, weight_decay=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(np.reshape(np.asarray(train_vector4),[1800,20,10]), np.asarray(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200, 2)]\n",
      "cnn is built!\n"
     ]
    }
   ],
   "source": [
    "prediction_cnn = cnn.predict(np.reshape(np.asarray(test_vector4),[200,20,10]))\n",
    "print('cnn is built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.64      0.16      0.26       100\n",
      "        pos       0.52      0.91      0.66       100\n",
      "\n",
      "avg / total       0.58      0.54      0.46       200\n",
      "\n",
      "0.535\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.asarray(test_labels), prediction_cnn))\n",
    "print(accuracy_score(np.asarray(test_labels), prediction_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
